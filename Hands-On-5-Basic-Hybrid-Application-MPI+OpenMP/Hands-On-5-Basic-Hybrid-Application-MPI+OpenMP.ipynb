{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97479be1",
   "metadata": {},
   "source": [
    "# Hands-On 5: Basic Hybrid Application MPI+OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306d61a-b2b7-4879-9382-91a27795c3af",
   "metadata": {},
   "source": [
    "This Hands-on comprises 2 session. Next table shows the documents and\n",
    "files needed to develop in the exercises.\n",
    "\n",
    "|  Sessions      | Codes                  | files              | \n",
    "| -------------- | -----------------------| ------------------ |\n",
    "| Session 1      | Hello World            | hello-mpi+openmp.c                             |\n",
    "| Session 2      | Hybrid Matrix Multiple | mm-mpi.c, mm-openmp.c, and mm-mpi+openmp.c |\n",
    "\n",
    "First, we will execute a sample hybrid code *Hello World* to understand the meaning of heterogeneous environment applied in Parallel Computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb9e17-e8a3-46b1-9350-83f18f482024",
   "metadata": {},
   "source": [
    "## `Hello World!` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58938bd3-d7d7-455e-a3f2-7ef5a86c3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hello-mpi+openmp.c\n",
    "#include <mpi.h>\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "\n",
    "int main( int argc, char *argv[])\n",
    "{\n",
    "    int nthreads,nprocs,idpro,idthr;\n",
    "    int  namelen;\n",
    "    char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "\n",
    "    MPI_Init(&argc,&argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD,&idpro);\n",
    "    MPI_Get_processor_name(processor_name,&namelen);\n",
    "\n",
    "    #pragma omp parallel private(idthr) firstprivate(idpro,processor_name)\n",
    "    {\n",
    "    idthr = omp_get_thread_num();\n",
    "    printf(\"Hello World  thread %d,From %d processor %s\\n\",idthr,idpro,processor_name);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e927d-a5ca-4291-849c-e745b30b0de2",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3231ee0-a0eb-4432-9bd1-da06a4ac14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc hello-mpi+openmp.c -o hello-mpi+openmp -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160eb51d-6164-44b8-b393-5c0889092d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMP_NUM_THREADS=2 mpirun -np 2 ./hello-mpi+openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86340f-ad2b-4a40-8a80-49a14f2d62f7",
   "metadata": {},
   "source": [
    "##  `Matrix Multiple Benchmarks`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9942dc9-ce58-4d80-aacd-6b8502b7644a",
   "metadata": {},
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0033c8-1e70-41ce-a9c0-94c2bfe87092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mm-openmp.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>\n",
    "\n",
    "void initializeMatrix(int *matrix, int size)\n",
    "{\n",
    "  for (int i = 0; i < size; i++)\n",
    "    for (int j = 0; j < size; j++)\n",
    "      matrix[i * size + j] = rand() % (10 - 1) * 1;\n",
    "}\n",
    "\n",
    "void printMatrix(int *matrix, int size)\n",
    "{\n",
    "  for (int i = 0; i < size; i++)\n",
    "  {\n",
    "    for (int j = 0; j < size; j++)\n",
    "      printf(\"%d\\t\", matrix[i * size + j]);\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main (int argc, char **argv)\n",
    "{\n",
    "\n",
    " int size = atoi(argv[1]);  \n",
    " int i, j, k;\n",
    " double t1, t2;\n",
    "\n",
    " int  *A = (int *) malloc (sizeof(int)*size*size);\n",
    " int  *B = (int *) malloc (sizeof(int)*size*size);\n",
    " int  *C = (int *) malloc (sizeof(int)*size*size);\n",
    "\n",
    " initializeMatrix(A, size);\n",
    " initializeMatrix(B, size);\n",
    "\n",
    " t1 = omp_get_wtime();\n",
    "\n",
    " #pragma omp parallel for private(i, j, k)\n",
    "   for(i = 0; i < size; i++)\n",
    "    for(j = 0; j < size; j++)\n",
    "      for(k = 0; k < size; k++)\n",
    "        C[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "\n",
    " t2 = omp_get_wtime();\n",
    "\n",
    " printf(\"%d\\t%f\\n\",size, t2-t1);\n",
    "\n",
    "// printMatrix(A,size);\n",
    "// printMatrix(B,size);\n",
    "// printMatrix(C,size);\n",
    "\n",
    " return 0;\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb6b97-9001-4c2c-a2be-bc3a6bd89bb5",
   "metadata": {},
   "source": [
    "#### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9420e-144c-41f4-86c8-97bb78b37c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc mm-openmp.c -o mm -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0432e-680e-4b0e-8c6e-83f6890e35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!MP_NUM_THREADS=16 ./mm  1000  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d42ea-c260-4704-ad92-434712e3a901",
   "metadata": {},
   "source": [
    "### MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a130b9-c3e6-4baf-9650-33a621157f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mm-mpi.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <sys/time.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "void mms(double *a, int fa, int ca, int lda, double *b, int fb, int cb, int ldb, double *c, int fc, int cc, int ldc) {\n",
    "    int i, j, k;\n",
    "    double s;\n",
    "    for (i = 0; i < fa; i++) \n",
    "        for (j = 0; j < cb; j++) {\n",
    "            s = 0.;\n",
    "            for (k = 0; k < ca; k++)\n",
    "                s += a[i * lda + k] * b[k * ldb + j];\n",
    "            c[i * ldc + j] = s;\n",
    "        }\n",
    "}\n",
    "\n",
    "void mm(double *a, int fa, int ca, int lda, double *b, int fb, int cb, int ldb, double *c, int fc, int cc, int ldc, int nodo, int np) {\n",
    "    int i, j, k;\n",
    "    double s;\n",
    "    if (nodo == 0) {\n",
    "        for (i = 1; i < np; i++)\n",
    "            MPI_Send(&a[i * lda * fa / np], fa / np * ca, MPI_DOUBLE, i, 20, MPI_COMM_WORLD);\n",
    "        MPI_Bcast(b, fb * cb, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "    } else {\n",
    "        MPI_Recv(a, fa / np * ca, MPI_DOUBLE, 0, 20, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        MPI_Bcast(b, fb * cb, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "    }\n",
    "    mms(a, fa / np, ca, lda, b, fb, cb, ldb, c, fc / np, cc, ldc);\n",
    "    if (nodo == 0)\n",
    "        for (i = 1; i < np; i++)\n",
    "            MPI_Recv(&c[i * ldc * fc / np],fc / np * cc, MPI_DOUBLE, i, 30, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    else\n",
    "        MPI_Send(c, fc / np * cc, MPI_DOUBLE, 0, 30, MPI_COMM_WORLD);\n",
    "}\n",
    "\n",
    "/*\n",
    "c\n",
    "c initialize - random initialization for array\n",
    "c\n",
    "*/\n",
    "\n",
    "void initialize(double *m, int f, int c, int ld) {\n",
    "  int i, j;\n",
    "\n",
    "  for (i = 0; i < f; i++) {\n",
    "    for (j = 0; j < c; j++) {  \n",
    "      m[i * ld + j] = (double)(i + j);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void initializealea(double *m, int f, int c, int ld) {\n",
    "  int i, j;\n",
    "\n",
    "  for (i = 0; i < f; i++) {\n",
    "    for (j = 0; j < c; j++) {  \n",
    "      m[i * ld + j] = (double)rand() / RAND_MAX;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void escribir(double *m, int f, int c, int ld) {\n",
    "  int i, j;\n",
    "\n",
    "  for (i = 0; i < f; i++) {\n",
    "    for (j = 0; j < c; j++) {  \n",
    "      printf(\"%.4lf \",m[i * ld + j]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "}\n",
    "\n",
    "void comparar(double *m1, int fm1, int cm1, int ldm1, double *m2, int fm2, int cm2, int ldm2)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < fm1; i++)\n",
    "    for(j = 0; j < cm1; j++) {\n",
    "      if(m1[i * ldm1 + j] != m2[i * ldm2 + j]) {\n",
    "        printf(\"Discrepance in %d,%d: %.8lf , %.8lf\\n\", i, j, m1[i * ldm1 + j], m2[i * ldm2 + j]);\n",
    "        return;\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  int nodo, np, i, j, fa, fal, ca, lda, fb, cb, ldb, fc, fcl, cc, ldc, N;\n",
    "  int long_name;\n",
    "  double ti, tf;\n",
    "  double *a, *b, *c, *c0;\n",
    "  char    nombre_procesador[MPI_MAX_PROCESSOR_NAME];\n",
    "  MPI_Status estado;\n",
    " \n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &nodo);\n",
    "  MPI_Get_processor_name(nombre_procesador, &long_name);\n",
    "\n",
    "  if (nodo == 0) {\n",
    "    N = atoi(argv[1]);\n",
    "  }\n",
    "  MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "  \n",
    "  fa = ca = lda = fb = cb = ldb = fc = cc = ldc = N;\n",
    "  fal = N / np;\n",
    "  fcl = N / np;\n",
    "  if (nodo == 0) {\n",
    "    a = (double *) malloc(sizeof(double) * fa * ca);\n",
    "    b = (double *) malloc(sizeof(double) * fb * cb);\n",
    "    c = (double *) malloc(sizeof(double) * fc * cc);\n",
    "  } else {\n",
    "    a = (double *) malloc(sizeof(double) * fal * ca);\n",
    "    b = (double *) malloc(sizeof(double) * fb * cb);\n",
    "    c = (double *) malloc(sizeof(double) * fcl * cc);\n",
    "  }\n",
    "  \n",
    "  if (nodo == 0) {\n",
    "    c0 = (double *) malloc(sizeof(double) * fc * cc);\n",
    "    initialize(a, fa, ca, lda);\n",
    "    initialize(b, fb, cb, ldb);\n",
    "\n",
    "    mms(a, fa, ca, lda, b, fb, cb, ldb, c0, fc, cc, ldc);\n",
    "  }\n",
    "\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "\n",
    "  ti = MPI_Wtime();\n",
    "\n",
    "  mm(a, fa, ca, lda, b, fb, cb, ldb, c, fc, cc, ldc, nodo, np);\n",
    "\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "  tf = MPI_Wtime();\n",
    "  if (nodo == 0) {\n",
    "    //printf(\"(%d) Process %d, %s, Time %.6lf\\n\", N, np, nombre_procesador, tf - ti);\n",
    "    printf(\"%d\\t%f\\n\", N, tf - ti);  \n",
    "  }\n",
    "  \n",
    "  free(a);\n",
    "  free(b);\n",
    "  free(c);\n",
    "  if (nodo == 0)\n",
    "    free(c0);\n",
    "  MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79767bf6-d378-41c5-9afc-8f899a7fd093",
   "metadata": {},
   "source": [
    "#### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e441b-0886-4f45-bbd8-d52bdedb4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc mm-mpi.c -o mm-mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07479d4-7303-4570-aa73-556cda58696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 ./mm-mpi  1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79d1cd-3f89-4300-bf23-e432ab1d3b8b",
   "metadata": {},
   "source": [
    "### MPI + OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37886f5e-5145-4b87-aac7-b92ae47552d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mm-mpi+openmp.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <sys/time.h>\n",
    "#include <mpi.h>\n",
    "#include <omp.h>\n",
    "\n",
    "void mm(double *a, int fa,int ca,int lda,double *b,int fb,int cb,int ldb,double *c,int fc,int cc,int ldc,int nodo,char *maquina)\n",
    "{\n",
    "  int i, j, k;\n",
    "  double s;\n",
    "\n",
    " #pragma omp parallel \n",
    " {\n",
    "   #pragma omp for private(i,j,k,s) schedule(static)\n",
    "    for (i = 0; i < fa; i++) \n",
    "    {\n",
    "     for(j=0;j<cb;j++)\n",
    "     {\n",
    "      s=0.;\n",
    "      for (k = 0; k < ca; k++)\n",
    "      s = s+a[i*lda+k]*b[k*ldb+j];\n",
    "      c[i*ldc+j]=s;\n",
    "     }\n",
    "    }\n",
    "   }\n",
    "  }\n",
    "\n",
    "/*\n",
    "c\n",
    "c initialize - random initialization for array\n",
    "c\n",
    "*/\n",
    "\n",
    "void initialize(double *m, int f,int c,int ld)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for (i = 0; i < f; i++)\n",
    "  {\n",
    "    for (j = 0; j < c; j++)\n",
    "    {  \n",
    "      m[i*ld+j] = (double)(i+j);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void initializealea(double *m, int f,int c,int ld)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for (i = 0; i < f; i++)\n",
    "  {\n",
    "    for (j = 0; j < c; j++)\n",
    "    {  \n",
    "      m[i*ld+j] = (double)rand()/RAND_MAX;\n",
    "    }\n",
    "  }\n",
    "    \n",
    "}\n",
    "\n",
    "void escribir(double *m, int f,int c,int ld)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for (i = 0; i < f; i++)\n",
    "  {\n",
    "    for (j = 0; j < c; j++)\n",
    "    {  \n",
    "      printf(\"%.4lf \",m[i*ld+j]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "    \n",
    "}\n",
    "/*\n",
    "c\n",
    "c     mseconds - returns elapsed milliseconds since Jan 1st, 1970.\n",
    "c\n",
    "*/\n",
    "long long mseconds(){\n",
    "  struct timeval t;\n",
    "  gettimeofday(&t, NULL);\n",
    "  return t.tv_sec*1000 + t.tv_usec/1000;\n",
    "}\n",
    "\n",
    "void comparar(double *m1,int fm1,int cm1,int ldm1,double *m2,int fm2,int cm2,int ldm2)\n",
    "{\n",
    "  int i,j;\n",
    "\n",
    "  for(i=0;i<fm1;i++)\n",
    "    for(j=0;j<cm1;j++)\n",
    "    {\n",
    "      if(m1[i*ldm1+j]!=m2[i*ldm2+j])\n",
    "      {\n",
    "        printf(\"Discrepance in %d,%d: %.8lf , %.8lf\\n\",i,j,m1[i*ldm1+j],m2[i*ldm2+j]);\n",
    "        return;\n",
    "      }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "int main(int argc,char *argv[]) \n",
    "{\n",
    "  int nodo,np,i, j,fa,fal,ca,lda,fb,cb,ldb,fc,fcl,cc,ldc,N,NUMTHREADS;\n",
    "  int long_name;\n",
    "  double ti,tf;\n",
    "  double *a,*b,*c,*c0;\n",
    "  char    nombre_procesador[MPI_MAX_PROCESSOR_NAME];\n",
    "  MPI_Status estado;\n",
    " \n",
    "  MPI_Init(&argc,&argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD,&np);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD,&nodo);\n",
    "  MPI_Get_processor_name(nombre_procesador,&long_name);\n",
    "\n",
    "  if(nodo==0)\n",
    "  {\n",
    "    N=atoi(argv[1]);\n",
    "    NUMTHREADS=atoi(argv[2]);\n",
    "  }\n",
    "    \n",
    "  MPI_Bcast(&N,1,MPI_INT,0,MPI_COMM_WORLD);\n",
    "  MPI_Bcast(&NUMTHREADS,1,MPI_INT,0,MPI_COMM_WORLD);\n",
    "  omp_set_num_threads(NUMTHREADS);\n",
    "  \n",
    "  fa=ca=lda=fb=cb=ldb=fc=cc=ldc=N;\n",
    "  fal=N/np;\n",
    "  fcl=N/np;\n",
    "  if(nodo==0)\n",
    "  {\n",
    "    a = (double *) malloc(sizeof(double)*fa*ca);\n",
    "    b = (double *) malloc(sizeof(double)*fb*cb);\n",
    "    c = (double *) malloc(sizeof(double)*fc*cc);\n",
    "  }\n",
    "  else\n",
    "  {\n",
    "    a = (double *) malloc(sizeof(double)*fal*ca);\n",
    "    b = (double *) malloc(sizeof(double)*fb*cb);\n",
    "    c = (double *) malloc(sizeof(double)*fcl*cc);\n",
    "  }\n",
    "  \n",
    "  if(nodo==0)\n",
    "  {\n",
    "    c0=(double *) malloc(sizeof(double)*fc*cc);\n",
    "    initialize(a,fa,ca,lda);\n",
    "    for(i=1;i<np;i++)\n",
    "    {\n",
    "      MPI_Send(&a[i*lda*N/np],fal*ca,MPI_DOUBLE,i,20,MPI_COMM_WORLD);\n",
    "    }\n",
    "    initialize(b,fb,cb,ldb);\n",
    "    MPI_Bcast(b,fb*cb,MPI_DOUBLE,0,MPI_COMM_WORLD);\n",
    "    mm(a,fa,ca,lda,b,fb,cb,ldb,c0,fc,cc,ldc,nodo,nombre_procesador);\n",
    "  }\n",
    "  else\n",
    "  {\n",
    "    MPI_Recv(a,fal*ca,MPI_DOUBLE,0,20,MPI_COMM_WORLD,&estado);\n",
    "    MPI_Bcast(b,fb*cb,MPI_DOUBLE,0,MPI_COMM_WORLD);\n",
    "  } \n",
    "\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "\n",
    "  ti=MPI_Wtime();\n",
    "\n",
    "  mm(a,fal,ca,lda,b,fb,cb,ldb,c,fcl,cc,ldc,nodo,nombre_procesador);\n",
    "\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "  tf=MPI_Wtime();\n",
    "  if(nodo==0)\n",
    "  {\n",
    "    printf(\"(%d) Threads %d, Process %d, %s, Time %.6lf\\n\\n\",N, NUMTHREADS, np, nombre_procesador,tf-ti);\n",
    "    printf(\"%d\\t%f\\n\", N, tf-ti);\n",
    "    for(i=1;i<np;i++)\n",
    "    {\n",
    "      MPI_Recv(&c[i*ldc*N/np],fcl*cc,MPI_DOUBLE,i,30,MPI_COMM_WORLD,&estado);\n",
    "    }\n",
    "  }\n",
    "  else\n",
    "  {\n",
    "    MPI_Send(c,fcl*cc,MPI_DOUBLE,0,30,MPI_COMM_WORLD);\n",
    "  } \n",
    "  \n",
    "  free(a);\n",
    "  free(b);\n",
    "  free(c);\n",
    "  if(nodo==0)\n",
    "    free(c0);\n",
    "  MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cb786-027c-4438-a69c-5e112075c475",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a825a-02d0-469d-9f65-60154efb9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc mm-mpi+openmp.c -o mm-mpi+openmp -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdb92f-3bca-4e5e-8405-336d37961a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 ./mm-mpi+openmp  1000 16     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db690918-4cf8-4b20-b45b-64bd07620315",
   "metadata": {},
   "source": [
    "## Practice with the Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9d5b9-3679-4d8c-a098-1775773723bf",
   "metadata": {},
   "source": [
    "From the parallelized matrix multiplication codes (MPI (`mm-mpi.c`), OpenMP (`mm-openmp.c)` and Hybrid (`mm-mpi+openmp.c`). Make the following plots:\n",
    "\n",
    "-   Execution time\n",
    "-   Speedup\n",
    "\n",
    "They are comparatively commenting on the performance of the optimizations of each code, with each group doing the experimentation on a specific execution platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353a617-6df7-43a7-af25-609415803ea5",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fb69f-263f-4d54-8657-a820e4dbef8e",
   "metadata": {},
   "source": [
    "M. Boratto. *Hands-On Supercomputing with Parallel Computing*.\n",
    "Available:\n",
    "<https://github.com/muriloboratto/Hands-On-Supercomputing-with-Parallel-Computing>.\n",
    "2022.\n",
    "\n",
    "Forum, Message Passing Interface. *MPI: A Message-Passing Interface\n",
    "Standard*. University of Tennessee, 1994, USA.\n",
    "\n",
    "B. Chapman, G. Jost and R. Pas. *Using OpenMP: Portable Shared Memory\n",
    "Parallel Programming*. The MIT Press, 2007, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
